---
name: zhi-wang-tinker-gpu-poster
title: "Tinker GPU: GPU-Accelerated MD Simulation Software for Advanced Force
Fields and Enhanced Sampling"
author: "Zhi Wang"
mentor-names: "Andrew Abi-Mansour"
full-author-list:
    - name: "Zhi Wang"
      affiliation: 1
    - name: "Jay W. Ponder"
      affiliation: 1
affiliations:
    - name: "Department of Chemistry, Washington University in St. Louis"
      address: "Saint Louis, MO 63130"
      index: 1
toc: true
toc_sticky: true
toc_label: "Poster Contents"
layout: poster
---

## Introduction

Advanced potential energy surfaces and models are being applied to an
ever-expanding array of problems in structural biology, drug design and
characterization of novel materials. These new potentials incorporate explicit
polarization effects and are calibrated very closely against energy
decompositions derived from electronic structure calculations. The Tinker
molecular modeling packages constitute one of the leading code families
implementing the AMOEBA (Atomic Multipole Optimized Energetics for Biomolecular
Applications) force field and many other advanced potential energy models.

As a new member of the Tinker suite, `tinker.gpu`
(get it from [GitHub](https://github.com/zhi-wang/tinker.gpu)) will focus on
accelerating the advanced force fields and enhanced sampling methods with novel
parallel algorithms on the state-of-the-art GPU devices.

![Tinker Logo]({{ site.url }}{{ site.baseurl }}/assets/images/2020-06-25-zhi-wang/mechanic.png){:width="400px"}
<center><i><b>Figure 1:</b> The Tinker logo.</i></center>

## Advanced Molecular Modeling

### Quadrupole Electrostatics

### Polarization

### Charge Transfer

### Dispersion and Repulsion

### Advanced Sampling Methods

## Code Structure

Even though `tinker.gpu` is written in modern templated C++, we will not
immediately give up our legacy Fortran code base. As a matter of fact, this
program still depends on the pre-built Fortran Tinker subroutines and modules
to manage the program I/O and global variables. The C++ is the *glue layer* to
communicate with Fortran library, to launch OpenACC/CUDA kernels at runtime,
and to perform unit tests.

![Code Structure]({{ site.url }}{{ site.baseurl }}/assets/images/2020-06-25-zhi-wang/code-struct.png){:width="600px"}
<center><i><b>Figure 2:</b> Code structure.</i></center>

### Legacy Fortran Subroutines and Modules

The interoperability of Fortran with C is of vital importance. Fortran
subroutines with no character string parameters can often be used directly in
C/C++ code with the help of the following macro

```cpp
// Fortran subroutine "getxyz" becomes "getxyz_"
// "call getxyz" becomes "TINKER_RT(getxyz)();" in C++
#define TINKER_RT(getxyz) getxyz##_
```

whereas for the global variables in the modules, different compiler vendors may
have different implementations. We used the following macro to access variable
`var` in module `mod`, for `gfortran` and `Intel ifort`, respectively

```cpp
// ...if gfortran
#define TINKER_MOD(mod, var) __##mod##_MOD_##var
// ...if Intel ifort
#define TINKER_MOD(mod, var) mod##_mp_##var##_
```

A syntax parser that can scan the Fortran Tinker source code and generate the
aforementioned bindings has been added to the source code.

An alternative solution would be exporting these symbols to C programs from
Fortran source code using `iso_c_binding` but requires extensive modification
in the legacy code base.

### Multi-precision and Multi-Platform

Our program can be configured (by [CMake](https://cmake.org) options)
to compile with single precision, double precision, and mixed precision
floating-point numbers, as well as to compile the *production GPU build*
or *reference CPU build*.

### Templated OpenACC and CUDA Kernels

Here are some of our principles in the GPU kernel developments that lead us to
the quadrupole kernel shown below.
* Don't repeat ourselves, especially for the energy/gradient/virial expressions.
* Don't pay for the things we don't need, e.g., skip energy evaluation in the
  simulation between saved MD frames.
* Maximal code reuse in multipole electrostatics.
* Minimal communication and synchronization between CPU and GPU.
* Always implement an OpenACC kernel first, unless we believe there will be
  significant improvement by CUDA.

```cuda
/**
 Pairwise quadrupole electrostatics.
 @tparam Version   Flag for using energy counts, potential energy,
                   virial tensor, and energy gradient in the pairwise
                   interaction.
 @tparam ElecType  Flag for using real space PME or Non-EWALD pairwise
                   interaction.
 */
template <class Version, class ElecType>
__device__
void pair_mpole(CUDA_KERNEL_PARAMETERS...)
{
   constexpr bool do_a = Version::a; // flag for number of interactions
   constexpr bool do_e = Version::e; // flag for potential energy
   constexpr bool do_v = Version::v; // flag for virial tensor
   constexpr bool do_g = Version::g; // flag for energy gradient

   // ...
}
```

### Unit Tests and Continuous Integration

We are using [Catch2](https://github.com/catchorg/Catch2) as the unit test
framework and [Travis CI](https://travis-ci.com/github/zhi-wang/tinker.gpu)
as the continuous integration platform for the reference CPU build.

## Conclusion
<!-- comparable to benchmark in JCTC paper -->

## References
1. Paper 1
2. Paper 2

## Acknowledgements

Zhi Wang was supported by a fellowship from The Molecular Sciences Software
Institute under NSF grant OAC-1547580.

![Tinker Heart]({{ site.url }}{{ site.baseurl }}/assets/images/2020-06-25-zhi-wang/heart.png){:width="400px"}
<center><i><b>Figure 3:</b> ❤️ Tinker.</i></center>
