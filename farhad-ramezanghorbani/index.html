<!DOCTYPE html>
<!--
  Minimal Mistakes Jekyll Theme 4.19.3 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
--><html lang="en" class="no-js">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Latent space representation learning as an auxiliary task for training neural network potentials - 2020 MolSSI Software Fellow Posters</title>
<meta name="description" content="Introduction">


  <meta name="author" content="Farhad Ramezanghorbani">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="2020 MolSSI Software Fellow Posters">
<meta property="og:title" content="Latent space representation learning as an auxiliary task for training neural network potentials">
<meta property="og:url" content="https://education.molssi.org/2020-software-fellow-posters/farhad-ramezanghorbani/">


  <meta property="og:description" content="Introduction">







  <meta property="article:published_time" content="2020-06-20T00:00:00+00:00">





  

  


<link rel="canonical" href="https://education.molssi.org/2020-software-fellow-posters/farhad-ramezanghorbani/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "The Molecular Sciences Software Institute",
      "url": "https://education.molssi.org/2020-software-fellow-posters/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/2020-software-fellow-posters/feed.xml" type="application/atom+xml" rel="alternate" title="2020 MolSSI Software Fellow Posters Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/2020-software-fellow-posters/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->
<link rel="stylesheet" href="https://use.typekit.net/wll4tkr.css">
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://unpkg.com/ngl@0.10.4/dist/ngl.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-169902961-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-169902961-1');
</script>


<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  <script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

  <body class="layout--poster">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/2020-software-fellow-posters/"><img src="/2020-software-fellow-posters/assets/images/avatars/molssi_avatar.png" alt=""></a>
        
        <a class="site-title" href="/2020-software-fellow-posters/">
          2020 MolSSI Software Fellow Posters
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/2020-software-fellow-posters/all_posters">Full Poster List</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/2020-software-fellow-posters/assets/images/avatars/farhad_ramezanghorbani.jpg" alt="Farhad Ramezanghorbani" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Farhad Ramezanghorbani</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Ph.D. Candidate at University of Florida</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="mailto:farhad.r@ufl.edu" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://farhadrgh.github.io/" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://twitter.com/FarhadRGhorbani" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
            <li><a href="https://github.com/farhadrgh" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/farhadrgh/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Latent space representation learning as an auxiliary task for training neural network potentials">
    <meta itemprop="description" content="Introduction">
    <meta itemprop="datePublished" content="2020-06-20T00:00:00+00:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Latent space representation learning as an auxiliary task for training neural network potentials
</h1>
          <div id="author-list" class="author__list">
            
              Farhad Ramezanghorbani<sup>1</sup>,
            
              Adrian E. Roitberg<sup>1</sup>
            
          </div>
    
          <div id="affiliation-list" class="affiliation__list">
            
              <div>
<strong>1</strong> Department of Chemistry, University of Florida, Gainesville, FL</div>
            
          </div>
       
        </header>
      

      
            <div id="mentor-name" class="mentor__list">
              <strong>MolSSI Mentor(s):</strong> Matthew Welborn, Levi N. Naden
            </div>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title">
<i class="fas fa-file-alt"></i> Poster Contents</h4></header>
              <ul class="toc__menu">
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#ani-network-architecture">ANI Network Architecture</a></li>
  <li><a href="#torchani">TorchANI</a></li>
  <li>
<a href="#multi-task-learning">Multi-Task Learning</a>
    <ul>
      <li><a href="#aniencoder-semi-supervised-representation-learning">ANIEncoder: Semi-supervised representation learning</a></li>
      <li><a href="#mtl-loss">MTL Loss</a></li>
    </ul>
  </li>
  <li><a href="#results-and-discussion">Results and discussion</a></li>
  <li><a href="#future-work">Future work</a></li>
  <li>
<a href="#references">References</a>
    <ul>
      <li><a href="#acknowledgements">Acknowledgements</a></li>
    </ul>
  </li>
</ul>

            </nav>
          </aside>
        
        <h2 id="introduction">Introduction</h2>

<p>Molecular properties can be obtained through a description of the electronic structure of any molecular system, which can be derived from high-level ab initio quantum mechanics (QM) by solving the Schrodinger equation. However, finding the exact solution to the many-body Schrodinger equation is NP-hard, or more specifically, QMA-hard (1), which means it is likely to be impossible to find the solution in polynomial time, even with quantum computers.</p>

<p>Various numerical approximations and computational techniques are proposed to accommodate the substantial cost of these calculations to generate results in a reasonable time-frame. These techniques have become integral for the early stages of drug discovery in cases where the selected methods need to match the fast pace of drug design settings in a cost-effective manner. A method that performs as fast as classical force fields and as accurate as high-level QM models is in high demand.</p>

<p>ANI (2) is a deep neural network potential trained to high precision QM methods to generate a transferable and extensible model that can predict molecular energies and forces. ANI potentials, in a suitable framework, can be applied to run dynamical simulations to quickly and accurately study the time evolution of biological systems.</p>

<h2 id="ani-network-architecture">ANI Network Architecture</h2>

<p>ANI uses atomic environment vectors (AEV)—a fixed-size representation of the local chemical environment, to predict the contribution of each atom to the total energy of the system. AEV symmetry functions are invariant under translation and rotation and were introduced by Behler and Parrinello (3). These functionals are a combination of two-body (radial) and three-body (angular) terms which encode information regarding the local environment of the atom up to a specific cutoff.</p>

<p><img src="https://education.molssi.org/2020-software-fellow-posters/assets/images/FARHAD_RAMEZANGHORBANI/AEV.png" alt="AEV"><br>
<em><strong>Figure 1</strong>: The Structure of the ANI AEVs. The sum of \(j\) and \(k\) is on all neighbor atoms of selected species/pair of species. \(R_s\) and \(\theta_s\) are hyper-parameters called radial/angular shifts. The \(f_C\) is called cutoff cosine function, defined as \(f_C(R)=\frac{1}{2}\left[\cos\left(\frac{\pi R}{R_C}\right) + 1\right]\) for \(R\leq R_C\) and \(0\) otherwise, where \(R_C\) is called cutoff radius, a hyperparameter that defines how far we should reach when investigating chemical environments. Image and caption taken from reference 4.</em></p>

<p>ANI Network architecture consists of a series of atomic feed-forward networks that input the coordinates \(q\) and atomic numbers \(Z\). Atomic AEVs are calculated and passed to the corresponding atomic network, which predicts their contribution to the molecular energy.</p>

<p><img src="https://education.molssi.org/2020-software-fellow-posters/assets/images/FARHAD_RAMEZANGHORBANI/aninn.png" alt="ANINN">
<em><strong>Figure 2</strong>: ANI Neural Network Architecture</em></p>

<h2 id="torchani">TorchANI</h2>
<p><a href="https://github.com/aiqm/torchani">TorchANI</a> (4) is an open-source PyTorch based implementation of ANI that provides a scalable framework for training and inference of deep learning models that benefits from an optimized chemical environment predictor.</p>

<p>TorchANI currently includes tools for inference and training of popular ANI-1x(5), ANI-1ccx(6), and ANI-2x(7) potentials. Predicting ANI energies and forces for a given molecule (e.g., methane) is easily achieved through the following few lines of code using TorchANI:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td>
<td class="code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchani</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torchani</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">ANI2x</span><span class="p">(</span><span class="n">periodic_table_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">coordinates</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.03</span><span class="p">,</span>  <span class="mf">0.006</span><span class="p">,</span>  <span class="mf">0.01</span><span class="p">],</span>
                             <span class="p">[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span>  <span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span>
                             <span class="p">[</span><span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">],</span>
                             <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span>   <span class="mf">0.5</span><span class="p">,</span>  <span class="mf">0.8</span><span class="p">],</span>
                             <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">]]],</span>
                           <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># atomic numbers for C, H, H, H, H
</span><span class="n">species</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">_</span><span class="p">,</span> <span class="n">energy</span> <span class="o">=</span> <span class="n">model</span><span class="p">((</span><span class="n">species</span><span class="p">,</span> <span class="n">coordinates</span><span class="p">))</span>
<span class="n">force</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">energy</span><span class="p">.</span><span class="nb">sum</span><span class="p">(),</span> <span class="n">coordinates</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Energy:'</span><span class="p">,</span> <span class="n">energy</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Force:'</span><span class="p">,</span> <span class="n">force</span><span class="p">.</span><span class="n">squeeze</span><span class="p">())</span>
</pre></td>
</tr></tbody></table></code></pre></figure>

<p>Moreover, PyTorch <a href="https://pytorch.org/docs/stable/autograd.html"><code class="language-plaintext highlighter-rouge">autograd</code></a> engine provides access to all the gradients in the computational graph. In the above example, the forces were predicted using the gradients of output molecular energy with respect to the input coordinates. Having access to the gradients one can compute a list of other physical properties such as hessian, vibrational frequencies, stress tensor, and infrared intensities.</p>

<p>TorchANI is interfaced with various computational chemistry packages, including the MolSSI QCArchive software stack, that uses ANI models for data generation via <a href="http://docs.qcarchive.molssi.org/projects/QCEngine/en/stable/index.html">QCEngine</a>.</p>

<p>TorchANI has a design emphasis on being lightweight, user-friendly, cross-platform, and easy to read/modify for fast prototyping of neural network architectures for molecular/atomic properties prediction. In the following sections, an example of such implementation—an AutoEncoder is explained.</p>

<h2 id="multi-task-learning">Multi-Task Learning</h2>
<p>Training a neural network to learn a set of tasks that could benefit from having shared low-level features, provides a considerable amount of knowledge to augment the otherwise relatively small examples for single-task training, a process known as implicit data augmentation.</p>

<p>Multi-Task Learning (MTL) can be combined with other learning paradigms, such as unsupervised and semi-supervised learning. Therefore, the vast amount of unlabeled data for training the unsupervised tasks can provide geometric information as an inductive bias to improve the performance of MTL(8), which is specifically advantageous in training neural network potentials where collecting labeled data is expensive (QM calculations), but unlabeled data are abundant.</p>

<h3 id="aniencoder-semi-supervised-representation-learning">ANIEncoder: Semi-supervised representation learning</h3>
<p>Autoencoding is to learn a mapping between a high dimensional space and a lower-dimensional manifold such that a subspace can be constructed given the distribution of the other subspace. Autoencoders are unsupervised frameworks that use an encoding network to learn a latent space representation (code), which is fed into a decoder network with the training objective of reproducing the original input data.</p>

<p>ANIEncoder (Figure 3) is a semi-supervised MTL architecture in which a representation is shared between a supervised task (i.e., the original ANI model), and an unsupervised task. AEV symmetry functionals have a high degree of sparsity and can be compressed into smaller representations. The encoding networks in ANIEncoder input AEVs to predict a latent space representation that could be interpreted as the low-dimensional atomic potential energy surface. The learned representation can be used to carry two separate objectives: A kernel that predicts the atomic contribution to the energies (the primary task), and a decoding network that learns to reproduce the input AEVs (the auxiliary task).</p>

<p><img src="https://education.molssi.org/2020-software-fellow-posters/assets/images/FARHAD_RAMEZANGHORBANI/aniae.png" alt="ANIAE">
<em><strong>Figure 3</strong>: ANIEncoder Architecture</em></p>

<p>The ANIEncoder architecture allows the incorporation of a vast amount of augmented data such as perturbed structures, low-level molecular dynamics trajectories, and noisy data. The network learns a smooth representation of multidimensional potential energy surface that will result in enhanced accuracy and generalization.</p>

<h3 id="mtl-loss">MTL Loss</h3>
<p>On-the-fly balancing of all the loss terms is required to ensure equal emphasis for training all objectives in an MTL problem such as ANIEncoder. Otherwise, one task might dominate the overall loss and consecutively converge faster while others cannot reach their learning potential and are unable to affect the learning process of the shared layers in a given number of optimization iterations.</p>

<p>The naive approach is to consider the overall loss as the weighted linear sum of task-specific loss functions (\(L_{i}\)). The total loss requires \(N\) hyperparameters \(\omega_i\) for an \(N\)-task problem. </p>

\[L_{total}\ =\sum_i^{N}\omega_iL_i\]

<p>However, the time and space complexity of finding the best set of hyperparameters using algorithms such as grid search substantially increases with the number of tasks in this case.</p>

<p>The task-specific prediction uncertainty can be employed to determine the relative confidence between different tasks (9, 10). The following mathematical formulation of this type of loss for MTL was introduced by Kendall et al. (9):</p>

<p>Let \(f\) be a neural network with parameters \(W\). Assuming Gaussian distribution for the underlying prediction error, the likelihood of prediction y is as follows:</p>

\[p(y|f^{W}(x)) = \mathcal{N}(f^{W}(x), \sigma^{2})\]

<p>The model can estimate both mean and variance \(\sigma^{2}\) (the observation noise) of output distribution given the input \(x\). In MTL problems where there exist multiple outputs of the model, the total likelihood becomes the multiplication of the task-specific probabilities. As an example for a two-task MTL:</p>

\[p(y_1, y_2 | f^{W}(x)) = p(y_1 | f^{W}(x)) \cdot p(y_2 | f^{W}(x))\]

\[= \mathcal{N}(y_1; f^{W}(x), \sigma_1^{2}) \cdot \mathcal{N}(y_2; f^{W}(x), \sigma_2^{2})\]

<p>By performing maximum a posteriori estimation (MAP) inference, we can obtain an objective function to minimize:</p>

<!-- $$ \log p(y|f^{W}(x)) \propto -\frac{1}{2\sigma^2} \||{y-f^{W}(x)}\||^{2} - \log \sigma $$ -->

\[-\log p(y_1, y_2 | f^{W}(x)) \propto \frac{1}{2\sigma_1^2} \||{y_1-f^{W}(x)}\||^{2} + \frac{1}{2\sigma_2^2} \||{y_2-f^{W}(x)}\||^{2} + \log \sigma_1 \sigma_2\]

\[= \frac{1}{2\sigma_1^2} \mathcal{L}_1(W) + \frac{1}{2\sigma_2^2} \mathcal{L}_2(W) + \log \sigma_1 \sigma_2\]

<p>Where \(\mathcal{L}_1\) and \(\mathcal{L}_2\) are the task-specific loss terms and can be replaced by squared error metrics often used as regression loss functions. Minimizing this MTL loss with respect to \(\sigma_{i}\) is equivalent to adaptively learning the relative weights of task-specific losses based on the data. The MTL loss was implemented for ANIEncoder to avoid using hyperparameters when training both energy learning and autoencoding tasks.</p>

<h2 id="results-and-discussion">Results and discussion</h2>
<p>ANIEncoder was trained on the ANI-1x dataset—a 5M DFT calculations of small organic molecules containing H, C, N, and O elements. The data were randomly split into %80/%20 training/validation. The network parameters for the autoencoding task were shared among all atomic networks and atomic energy kernels predict their contribution to the molecular energies using the representation learned in the unsupervised task. The evaluation results of ANIEncoder on <a href="https://github.com/isayev/COMP6">COMP6</a> benchmark are provided in Table 1.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Energy</th>
      <th>Relative Energy</th>
      <th>Forces</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ANI-1x</td>
      <td>1.93/3.37</td>
      <td>1.85/2.95</td>
      <td>3.09/5.29</td>
    </tr>
    <tr>
      <td>ANIEncoder</td>
      <td>2.17/3.35</td>
      <td>2.19/3.37</td>
      <td>3.98/6.29</td>
    </tr>
  </tbody>
</table>

<p><em><strong>Table 1</strong>: COMP6 benchmark (MAE/RMSE) results in  kcal/mol (energies) and kcal/mol/A (forces)</em></p>

<p>The ANIEncoder MAE/RMSE metrics for the COMP6 dataset closely reproduce the ANI-1x model trained with the same dataset. However, hyperparameter tuning can help achieve identical or better performance. The training was found to be sensitive to the learning rate decay and the number of shared layers.</p>

<p>The MTL loss balances the learning pace of energy prediction and autoencoding tasks on the fly. However, data imbalance can bias the representation learning of different elements. The abundance of Hydrogens in our datasets biases the learning process. We can adjust the learning rate, the number of parameters, or regularizations applied to individual atomic energy kernels to account for the data imbalance. However, the representation learning task cannot distinguish the element types and therefore is biased towards Hydrogens.</p>

<h2 id="future-work">Future work</h2>

<p>ANIEncoder was trained on the ANI-1x dataset in which all the data points are labeled with their DFT energies. However, the advantage of this semi-supervised model is the ability to use unlabeled data, i.e., molecules without the QM energy labels. One way to achieve this is by adding random multivariate gaussian noise to the existing coordinates or perturbing the structure in the direction of the normal modes (normal mode sampling).</p>

<p>The amount of parameter sharing between different tasks can be adjusted depending on their similarity and constructiveness. We can decrease the amount of sharing if simultaneously learning multiple tasks is degrading the performance.</p>

<p>The experimental version of ANIEncoder will be publicly released after passing accuracy tests.</p>

<h2 id="references">References</h2>
<ol>
  <li>Aaronson, S. (2009). Why quantum chemistry is hard. Nature Physics, 5(10), 707-708.</li>
  <li>Smith, J. S., Isayev, O., &amp; Roitberg, A. E. (2017). ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost. Chemical science, 8(4), 3192-3203.</li>
  <li>Behler, J., &amp; Parrinello, M. (2007). Generalized neural-network representation of high-dimensional potential-energy surfaces. Physical review letters, 98(14), 146401.</li>
  <li>Gao, X., Ramezanghorbani, F., Isayev, O., Smith, J., &amp; Roitberg, A. (2020). TorchANI: <a href="https://pubs.acs.org/doi/10.1021/acs.jcim.0c00451">A Free and Open Source PyTorch Based Deep Learning Implementation of the ANI Neural Network Potentials. Journal of chemical information and modeling.</a>
</li>
  <li>Smith, J. S., Nebgen, B., Lubbers, N., Isayev, O., &amp; Roitberg, A. E. (2018). Less is more: Sampling chemical space with active learning. The Journal of chemical physics, 148(24), 241733.</li>
  <li>Smith, J. S., Nebgen, B. T., Zubatyuk, R., Lubbers, N., Devereux, C., Barros, K., … &amp; Roitberg, A. E. (2019). Approaching coupled cluster accuracy with a general-purpose neural network potential through transfer learning. Nature communications, 10(1), 1-8.</li>
  <li>Devereux, C., Smith, J., Davis, K., Barros, K., Zubatyuk, R., Isayev, O., … &amp; Isayev, O. (2020). Extending the applicability of the ANI deep learning molecular potential to Sulfur and Halogens. ChemRxiv.</li>
  <li>Caruana, R. (1997). Multitask learning. Machine learning, 28(1), 41-75.</li>
  <li>Kendall, A., Gal, Y., &amp; Cipolla, R. (2018). Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7482-7491).</li>
  <li>Scalia, G., Grambow, C. A., Pernici, B., Li, Y. P., &amp; Green, W. H. (2020). Evaluating Scalable Uncertainty Estimation Methods for Deep Learning-Based Molecular Property Prediction. Journal of Chemical Information and Modeling.</li>
</ol>

<h3 id="acknowledgements">Acknowledgements</h3>
<p>“Farhad Ramezanghorbani was supported by a fellowship from The Molecular Sciences Software Institute under NSF grant OAC-1547580”</p>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2020-06-20T00:00:00+00:00">June 20, 2020</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Latent+space+representation+learning+as+an+auxiliary+task+for+training+neural+network+potentials%20https%3A%2F%2Feducation.molssi.org%2F2020-software-fellow-posters%2Ffarhad-ramezanghorbani%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Feducation.molssi.org%2F2020-software-fellow-posters%2Ffarhad-ramezanghorbani%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Feducation.molssi.org%2F2020-software-fellow-posters%2Ffarhad-ramezanghorbani%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/2020-software-fellow-posters/heta-gandhi/" class="pagination--pager" title="Using Machine Learning to Improve Coarse-Grained Simulations
">Previous</a>
    
    
      <a href="/2020-software-fellow-posters/lauren-koulias/" class="pagination--pager" title="Relativistic Excited State Coupled-Cluster
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2020-software-fellow-posters/vh-chavez/" rel="permalink">PDFT - An accessible density embedding code
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="author">Victor H. Chavez</p>
    <p class="archive__item-excerpt" itemprop="description">Introduction

</p>
  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2020-software-fellow-posters/sebastian-lee/" rel="permalink">Simple interoperability via a general representation of molecular wavefunctions
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="author">Sebastian J. R. Lee</p>
    <p class="archive__item-excerpt" itemprop="description">Background and Motivation

</p>
  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2020-software-fellow-posters/giuseppe-barbalinardo/" rel="permalink">Efficient Anharmonic Lattice Dynamics Calculations of Thermal Transport in Crystalline and Disordered Solids using kALDo
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="author">Giuseppe Barbalinardo</p>
    <p class="archive__item-excerpt" itemprop="description">
  kALDo is a versatile and scalable open-source software to compute phonon transport in crystalline and amorphous solids. It features real space QHGK calcul...</p>
  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/2020-software-fellow-posters/zhi-wang/" rel="permalink">Tinker GPU: GPU-Accelerated MD Simulation Software for Advanced Force Fields and Enhanced Sampling
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="author">Zhi Wang</p>
    <p class="archive__item-excerpt" itemprop="description">Introduction
Advanced potential energy surfaces and models are being applied to an
ever-expanding array of problems in structural biology, drug design and
ch...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/MolSSI_NSF" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/molssi" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://molssi.org/" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i> Website</a></li>
        
      
    

    <li><a href="/2020-software-fellow-posters/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">© 2020 The Molecular Sciences Software Institute. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/2020-software-fellow-posters/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>










  </body>
</html>
